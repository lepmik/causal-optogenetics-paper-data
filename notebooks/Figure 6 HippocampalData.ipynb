{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of IV and CCH method on hippocampal data with optical stimulation of pyramidal neurons\n",
    "\n",
    "Data kindly provided by Sam McKenzie and Daniel Fine English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tools_analysis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-164a4d1cee12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSpecFromSubplotSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/instrumentalVariable/causal-optoconnectics/method.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSpecFromSubplotSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from tools_analysis import (correlogram, poisson_continuity_correction,\n\u001b[0m\u001b[1;32m      6\u001b[0m                             cch_convolve)\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tools_analysis'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from method import IV\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n",
    "import numpy as np\n",
    "import quantities as pq\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('../exana/'),\n",
    "from exana.stimulus import plot_psth\n",
    "from exana.statistics.plot import plot_xcorr, plot_autocorr\n",
    "from exana.statistics.tools import ccg_significance, correlogram, ccg\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import neo\n",
    "import exana\n",
    "\n",
    "import pdb\n",
    "from scipy.ndimage.filters import gaussian_filter1d as gaussfilt\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import tools_experimentaldata as tls_exp\n",
    "\n",
    "from tools_plot import savefig, fix_figure, set_style, despine\n",
    "import statsmodels.api as sm\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage  \n",
    "from scipy import fftpack\n",
    "from exana.statistics.tools import hollow_kernel as hk\n",
    "from itertools import chain\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "## for Palatino and other serif fonts use:\n",
    "#rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_style(style='article', sns_style='white', w=1, h=1):\n",
    "    sdict = {\n",
    "        'article': {\n",
    "            # (11pt font = 360pt, 4.98) (10pt font = 345pt, 4.77)                                                                                                                                                                                                                         \n",
    "            'figure.figsize' : (4.98 * w, 2 * h),\n",
    "            'figure.autolayout': False,\n",
    "            'lines.linewidth': 2,\n",
    "            'font.size'      : 11,\n",
    "            'legend.frameon' : False,\n",
    "            'legend.fontsize': 11,\n",
    "            'font.family'    : 'serif',\n",
    "            'text.usetex'    : True\n",
    "        },\n",
    "        'notebook': {\n",
    "            'figure.figsize' : (16, 9),\n",
    "            'axes.labelsize' : 50,\n",
    "            'lines.linewidth': 4,\n",
    "            'lines.markersize': 20,\n",
    "            'xtick.labelsize': 30,\n",
    "            'ytick.labelsize': 30,\n",
    "            'axes.titlesize' : 20,\n",
    "            'font.size'      : 20,\n",
    "            'legend.frameon' : False,\n",
    "            'legend.fontsize': 35,\n",
    "            'font.family'    : 'serif',\n",
    "            'text.usetex'    : True\n",
    "        }\n",
    "    }\n",
    "    rc = sdict[style]\n",
    "    plt.rcParams.update(rc)\n",
    "    sns.set(rc=rc, style=sns_style,\n",
    "            color_codes=True)\n",
    "set_style('article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load params\n",
    "from params_experimentaldata import *\n",
    "# update figure settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_db = tls_exp.load_unitlabels('optoLabels.mat', data_dir)\n",
    "df_tmp = units_db.drop_duplicates(['animal', 'date'])\n",
    "relevant_data = df_tmp.groupby('animal')['date'].apply(list).to_dict()\n",
    "\n",
    "for animal_i in relevant_data.keys():\n",
    "    for date_i in relevant_data[animal_i]:\n",
    "        for entry in blk_blacklist:\n",
    "            if animal_i == entry['animal'] and date_i == entry['date']:\n",
    "                relevant_data[animal_i].remove(entry['date'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_orig_files = True\n",
    "\n",
    "if load_orig_files:\n",
    "    tls_exp.download_files_by_dict(relevant_data,\n",
    "                                   data_dir,\n",
    "                                   n_shanks,\n",
    "                                   files_ext_general,\n",
    "                                   files_ext_by_shank,\n",
    "                                   link_db)\n",
    "\n",
    "    blks = tls_exp.create_neo_structure(relevant_data,\n",
    "                                        data_dir,\n",
    "                                        n_shanks,\n",
    "                                        sampling_rate,\n",
    "                                        unit_spiketime)\n",
    "\n",
    "    tls_exp.add_stimulation_data_to_blocks(blks)\n",
    "    tls_exp.annotate_units_from_db(units_db, blks)\n",
    "    for blk in blks:\n",
    "        animal = blk.annotations['animal']\n",
    "        date = blk.annotations['date']\n",
    "        nio = neo.io.PickleIO(data_dir + 'neo_files/' + animal + '_' + date + '.pckl')\n",
    "        nio.write_block(blk)\n",
    "\n",
    "else:\n",
    "    blks = []\n",
    "    for animal in relevant_data.keys():\n",
    "        for date in relevant_data[animal]:\n",
    "            nio = neo.io.PickleIO(data_dir + 'neo_files/' + animal + '_' + date + '.pckl')\n",
    "            blk = nio.read_block()\n",
    "            blks.append(blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blks = tls_exp.select_blocks_upon_stimtype(blks,\n",
    "    stimtype='pulse', min_intens=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine which stimulations intensities have a significant effect  on units\n",
    "We a) group very similar stimulations intensities and b) test whether intensity group has a significant effect on increasing spiking probability of any of the given units by convolving stimulation onsets with spike train.\n",
    "We use the activity before a stimulation as baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blks = tls_exp.group_stimulations(blks,\n",
    "                                  sep_bins,\n",
    "                                  sep_kernel_width,\n",
    "                                  sep_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stim = tls_exp.find_significant_stimulations(blks,\n",
    "   stimccg_binsize,\n",
    "   stimccg_limit,\n",
    "   stimccg_pthres,\n",
    "   condition_annot_unit={'tagged': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example plot of  stimulation response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "row_sel = 2\n",
    "ln0 = ax.plot(df_stim.loc[row_sel]['bins'],\n",
    "        df_stim.loc[row_sel]['cch'],\n",
    "       label='count')\n",
    "ln1 = ax.axhline(df_stim.loc[row_sel]['rate_baseline'],\n",
    "           label='baseline',\n",
    "          linestyle='--')\n",
    "ax.set_xlabel(r'$\\Delta t$')\n",
    "ax.set_ylabel(r'count')\n",
    "ax2 = ax.twinx()\n",
    "ln2 = ax2.plot(df_stim.loc[row_sel]['bins'],\n",
    "         df_stim.loc[row_sel]['pfast'], c='r',\n",
    "        label='prob')\n",
    "ln3 = ax2.axhline(stimccg_pthres, c='r', linestyle='--',alpha=0.5, label='sign. level')\n",
    "ax2.set_ylabel(r'prob')\n",
    "\n",
    "ax2.set_ylim(0, 0.1)\n",
    "ax.set_xlim(-50., 50.)\n",
    "ax2.set_xlim(-50., 50.)\n",
    "\n",
    "ln = ln0+[ln1]+ln2+[ln3]\n",
    "labs = [l.get_label() for l in ln]\n",
    "ax.legend(ln, labs, loc=0)\n",
    "ax.set_title('Example CCH between stimulus onset and spikes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of stimulation intensities\n",
    "For each unit, we show the strongest available stimulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "group_obj = df_stim.groupby(['animal', 'date', 'shank_unit', 'cluster', 'shank_stim'])\n",
    "\n",
    "idxmax = group_obj['intens_mean'].idxmax()\n",
    "idxmin = group_obj['intens_mean'].idxmin()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_obj['intens_mean'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response time by stimulation intensity\n",
    "We visualize the time it takes for units to show a significant increase in spiking probability for maximal stimulation intensities on same shank vs other shanks.\n",
    "The last bin, $\\Delta t = 25$ ms, includes also larger times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max_same = []\n",
    "t_max_diff = []\n",
    "intens_max_same =[]\n",
    "intens_max_diff =[]\n",
    "for i in idxmax:\n",
    "    if df_stim.loc[i]['shank_stim'] == df_stim.loc[i]['shank_unit']:\n",
    "        t_max_same.append(df_stim.loc[i]['first_bin_sig'])\n",
    "        intens_max_same.append(df_stim.loc[i]['intens_mean'])\n",
    "    if df_stim.loc[i]['shank_stim'] != df_stim.loc[i]['shank_unit']:\n",
    "        t_max_diff.append(df_stim.loc[i]['first_bin_sig'])\n",
    "        intens_max_diff.append(df_stim.loc[i]['intens_mean'])\n",
    "t_max_same = np.array(t_max_same)\n",
    "t_max_diff = np.array(t_max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min_same = []\n",
    "t_min_diff = []\n",
    "intens_min_same =[]\n",
    "intens_min_diff =[]\n",
    "for i in idxmin:\n",
    "    if df_stim.loc[i]['shank_stim'] == df_stim.loc[i]['shank_unit']:\n",
    "        t_min_same.append(df_stim.loc[i]['first_bin_sig'])\n",
    "        intens_min_same.append(df_stim.loc[i]['intens_mean'])\n",
    "    if df_stim.loc[i]['shank_stim'] != df_stim.loc[i]['shank_unit']:\n",
    "        t_min_diff.append(df_stim.loc[i]['first_bin_sig'])\n",
    "        intens_min_diff.append(df_stim.loc[i]['intens_mean'])\n",
    "t_min_same = np.array(t_min_same)\n",
    "t_min_diff = np.array(t_min_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,3))\n",
    "bins = np.arange(1.5, 31.5, 3)\n",
    "hist, bins = np.histogram(t_max_same, bins)\n",
    "hist = hist / len(t_max_same)\n",
    "ax.bar(bins[:-1], hist)\n",
    "despine(ax)\n",
    "ax.set_title('Time to significant response upon stimulation with maximal intensity on same shank')\n",
    "ax.set_xlabel(r'$\\Delta t$ stimulation onset [ms]')\n",
    "ax.set_ylabel('Fraction of units')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,3))\n",
    "bins = np.arange(1.5, 31.5, 3)\n",
    "hist, bins = np.histogram(t_min_diff, bins)\n",
    "hist = hist / len(t_min_diff)\n",
    "ax.bar(bins[:-1], hist)\n",
    "despine(ax)\n",
    "ax.set_title('Time to significant response upon stimulation with minimal intensity on different shank')\n",
    "ax.set_xlabel(r'$\\Delta t$ stimulation onset [ms]')\n",
    "ax.set_ylabel('Fraction of units')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Stimulation on a different shank leads only in very few cases to a significant reaction in a reasonable time frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate IV\n",
    "We calculate the wald estimate ofor those stimulations that show a significant response within the IV window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windw = iv_window.rescale(df_stim['first_bin_sig'].values[0].units).magnitude[np.newaxis][0]\n",
    "df_sigstim = df_stim[df_stim['first_bin_sig'] <= windw]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sigstim['intens_mean'] = df_sigstim['intens_mean'].astype(int)\n",
    "group_sigstim = df_sigstim.groupby(['animal', 'date', 'shank_unit', 'cluster', 'shank_stim'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv = tls_exp.calculate_iv_sigstim(\n",
    "    blks,\n",
    "    df_sigstim,\n",
    "    iv_min_n_stim,\n",
    "    iv_window,\n",
    "    iv_ltnc,\n",
    "    condition_annot_pre={'tagged': True},\n",
    "    condition_annot_post={'tagged': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate CCH estimate\n",
    "We calculate the so called transmission probability, the CCH estimate of synaptic coupling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cch_all = tls_exp.calculate_transmission_prob(\n",
    "    blks,\n",
    "    ccg_time_limit,\n",
    "    ccg_binsize,\n",
    "    ccg_hollow_fraction,\n",
    "    ccg_width,\n",
    "    ccg_sig_level_causal,\n",
    "    ccg_sig_level_fast,\n",
    "    ccg_peak_wndw,\n",
    "    condition_annot_pre={'tagged': True},\n",
    "    condition_annot_post={'tagged': False})\n",
    "df_cch_all.rename(columns={'transprob': 'transproball',\n",
    "                            'bool_cnnctd': 'boolcnnctdall'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For curiosity we do it also for only spikes that arised spontaneously and the first evoked spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blks_nostim = tls_exp.keep_spikes_by_stim(blks, keep='nostim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cch_nostim = tls_exp.calculate_transmission_prob(\n",
    "    blks_nostim,\n",
    "    ccg_time_limit,\n",
    "    ccg_binsize,\n",
    "    ccg_hollow_fraction,\n",
    "    ccg_width,\n",
    "    ccg_sig_level_causal,\n",
    "    ccg_sig_level_fast,\n",
    "    ccg_peak_wndw, \n",
    "    condition_annot_pre={'tagged': True},\n",
    "    condition_annot_post={'tagged': False})\n",
    "df_cch_nostim.rename(\n",
    "    columns={'transprob': 'transprobspont',\n",
    "             'bool_cnnctd': 'boolcnnctdspont'},\n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    blks_stim = tls_exp.select_only_first_spike(\n",
    "        blks,\n",
    "        condition_annot_unit={'tagged': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cch_stim = tls_exp.calculate_transmission_prob(\n",
    "    blks_stim,\n",
    "    ccg_time_limit,\n",
    "    ccg_binsize,\n",
    "    ccg_hollow_fraction,\n",
    "    ccg_width,\n",
    "    ccg_sig_level_causal,\n",
    "    ccg_sig_level_fast,\n",
    "    ccg_peak_wndw,\n",
    "    condition_annot_pre={'tagged': True},\n",
    "    condition_annot_post={'tagged': False})\n",
    "df_cch_stim.rename(columns={'transprob': 'transprobevoked',\n",
    "                            'bool_cnnctd': 'boolcnnctdevoked'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cch = pd.merge(df_iv, df_cch_nostim,\n",
    "    on=['animal','date',\n",
    "        'shank_pre', 'cluster_pre',\n",
    "       'shank_post','cluster_post'])\n",
    "df_cch = pd.merge(df_cch, df_cch_stim,\n",
    "    on=['animal','date',\n",
    "        'shank_pre', 'cluster_pre',\n",
    "        'shank_post','cluster_post'])\n",
    "#df_cch = pd.merge(df_iv, df_cch_stim,\n",
    "#    on=['animal','date',\n",
    "#        'shank_pre', 'cluster_pre',\n",
    "#       'shank_post','cluster_post'])\n",
    "df_cch = pd.merge(df_cch, df_cch_all,\n",
    "    on=['animal','date',\n",
    "        'shank_pre', 'cluster_pre',\n",
    "        'shank_post','cluster_post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5,5))\n",
    "\n",
    "#ax.set(xscale=\"log\", yscale=\"log\")\n",
    "\n",
    "ax.scatter(df_cch.loc[df_cch['boolcnnctdall']==True]['transproball'],\n",
    "           df_cch.loc[df_cch['boolcnnctdall']==True]['ivwald'],\n",
    "          c='b',\n",
    "          alpha=0.4)\n",
    "#ax.scatter(df_cch['transproball'],\n",
    "#           df_cch['ivwald'],\n",
    "#          c='r',\n",
    "#          alpha=0.1)\n",
    "ax.set_xlabel(r'CCH')\n",
    "ax.set_ylabel(r'IV')\n",
    "ax.set_xlim([-0.0001, 0.011])\n",
    "#ax.set_ylim([-0.03, 0.125])\n",
    "despine(ax)\n",
    "ax.set_title('Comparison IV and CCH on all spikes')\n",
    "plt.show()\n",
    "fig.tight_layout(rect=[0, 0.00, 1, 1])\n",
    "#fig.savefig('manuscript/Optodata_comparisonIV_CCH.svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5,5))\n",
    "\n",
    "#ax.set(xscale=\"log\", yscale=\"log\")\n",
    "\n",
    "ax.scatter(df_cch.loc[df_cch['boolcnnctdspont']==True]['transprobspont'],\n",
    "           df_cch.loc[df_cch['boolcnnctdspont']==True]['ivwald'],\n",
    "          c='b',\n",
    "          alpha=0.4)\n",
    "ax.set_xlim([-0.0001, 0.011])\n",
    "ax.set_ylim([-0.03, 0.125])\n",
    "ax.set_xlabel(r'CCH spontaneous')\n",
    "ax.set_ylabel(r'iv wald')\n",
    "ax.set_title('Comparison IV and CCH on spontaneous spikes')\n",
    "despine(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5,5))\n",
    "\n",
    "#ax.set(xscale=\"log\", yscale=\"log\")\n",
    "\n",
    "ax.scatter(df_cch.loc[df_cch['boolcnnctdevoked']==True]['transprobevoked'],\n",
    "           df_cch.loc[df_cch['boolcnnctdevoked']==True]['ivwald'],\n",
    "          c='b',\n",
    "          alpha=0.4)\n",
    "ax.set_xlim([-0.0001, 0.011])\n",
    "ax.set_ylim([-0.03, 0.125])\n",
    "ax.set_xlabel(r'CCH spontaneous')\n",
    "ax.set_ylabel(r'iv wald')\n",
    "ax.set_title('Comparison IV and CCH on first evoked spike only')\n",
    "despine(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df_pre = df_cch[\n",
    "    ['animal', 'date', 'shank_pre', 'cluster_pre']\n",
    "].groupby(['animal', 'date', 'shank_pre']).nunique()\n",
    "group_df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df_post = df_cch[\n",
    "    ['animal', 'date', 'shank_post', 'cluster_post']\n",
    "].groupby(\n",
    "    ['animal', 'date', 'shank_post'])\n",
    "group_df_post.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('session_expdata.tex','w') as tf:\n",
    "#    tf.write(group_sigstim['intens_mean'].unique().to_latex())\n",
    "group_sigstim['intens_mean'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of IV and CCH estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "res =tls_exp.regplot('transproball', 'ivwald',\n",
    "                df_cch.loc[df_cch['boolcnnctdall']==True],\n",
    "                sm.OLS, colorbar=False, xlabel=r'CCH',\n",
    "                ylabel=r'IV', ax=ax)\n",
    "print('pValue: ' + str(res.f_pvalue))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cch.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "We find that there is a weak correlation between iv and the cch estimate of $0.24$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation of presynaptic units\n",
    "Because of slow stimulation onset we used a relatively long iv window.\n",
    "We want to see how this compares to the refractory period of pyramidal cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorr_dict = {}\n",
    "units_pre = df_cch.loc[df_cch['boolcnnctdall']==True].groupby(\n",
    "    ['animal', 'date', 'shank_pre', 'cluster_pre']\n",
    ").apply(list).to_dict()\n",
    "for animal, date, shank_pre, cluster_pre in units_pre.keys():\n",
    "    for blk in blks:\n",
    "        units = blk.channel_indexes[0].children\n",
    "        if blk.annotations['date'] == date and blk.annotations['animal'] == animal:\n",
    "            unit_i = [unit for unit in units if\n",
    "                      unit.annotations['shank'] == shank_pre and\n",
    "                      unit.annotations['cluster'] == cluster_pre][0]\n",
    "            spktr=unit_i.spiketrains[0]\n",
    "            cnt, bins_autocorr = correlogram(\n",
    "                spktr, auto=True,\n",
    "                limit=autocorr_limit,\n",
    "                binsize=autocorr_binsize,\n",
    "                density=True)\n",
    "            autocorr_dict[animal + '_' + date + '_' \n",
    "                          +str(shank_pre) +'_'+ str(cluster_pre)] = cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorr = np.array(list(chain(autocorr_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "autocorr_mean = np.mean(autocorr, axis=0)\n",
    "autocorr_std = np.std(autocorr, axis=0)\n",
    "\n",
    "ax.plot(bins_autocorr, autocorr_mean)\n",
    "ax.plot(bins_autocorr, autocorr_mean+autocorr_std, alpha=0.1)\n",
    "ax.plot(bins_autocorr, autocorr_mean-autocorr_std, alpha=0.1)\n",
    "\n",
    "ax.set_ylabel(r'Mean of normalized counts')\n",
    "ax.set_xlabel(r'Offset')\n",
    "despine(ax)\n",
    "savefig(fig, fname='Optodata_autocorrelation.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "The average of the scaled autocorrelogram indicates a refractory period of around $4 ms$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "To test the uncertainty of each IV and CCH estimate, we perform bootstrapping with a sample size of $1000$\n",
    "\n",
    "#### CCH\n",
    "We use the additive property of the cross correlation function.\n",
    "We calculate the CCH on chunks of spike trains.\n",
    "We then randomly pick chunks with replacement, add them up and perform CCH estimation.\n",
    "\n",
    "#### IV\n",
    "We first find all onsets and classify whether they contain a spike or not.\n",
    "Then we randomly pick onsets with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider only significant connections\n",
    "# for bootstrapping\n",
    "conns_sel = list(\n",
    "    df_cch.loc[df_cch['boolcnnctdall']==True].groupby(\n",
    "    ['animal', 'date', 'shank_pre', 'cluster_pre', 'shank_post', 'cluster_post']\n",
    "    ).apply(list).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculate_df_btstrp_cch = True\n",
    "\n",
    "if calculate_df_btstrp_cch:\n",
    "    df_btstrp_cch = tls_exp.bootstrap_cch(\n",
    "            blks, btstrp_n,\n",
    "            btstrp_binsize,                                                                                                                                                                                            \n",
    "            ccg_binsize,                                                                                                                                                                                               \n",
    "            ccg_hollow_fraction,                                                                                                                                                                                       \n",
    "            ccg_peak_wndw,                                                                                                                                                                                             \n",
    "            ccg_time_limit,\n",
    "            ccg_width,\n",
    "            conns_sel=conns_sel)\n",
    "    df_btstrp_cch.to_pickle('df_btstrp_cch')\n",
    "else:\n",
    "    df_btstrp_cch = pd.read_pickle('df_btstrp_cch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculate_df_btstrp_iv = True\n",
    "\n",
    "if calculate_df_btstrp_iv:\n",
    "    df_btstrp_iv = tls_exp.bootstrap_iv(\n",
    "            blks, btstrp_n,                                                                                                                                                                                                   \n",
    "            df_sigstim,                                                                                                                                                                                                \n",
    "            iv_min_n_stim,                                                                                                                                                                                             \n",
    "            iv_window,                                                                                                                                                                                                 \n",
    "            iv_ltnc,                                                                                                                                                                                                   \n",
    "            conns_sel=conns_sel)\n",
    "    df_btstrp_iv.to_pickle('df_btstrp_iv')\n",
    "else:\n",
    "    df_btstrp_iv = pd.read_pickle('df_btstrp_iv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI = 66.6\n",
    "perc_low = (100-CI)/2\n",
    "perc_high = 100 - perc_low\n",
    "\n",
    "cch_err_low = []\n",
    "cch_err_high = []\n",
    "\n",
    "iv_err_low = []\n",
    "iv_err_high = []\n",
    "\n",
    "iv_med = []\n",
    "cch_med = []\n",
    "\n",
    "for conn in conns_sel:\n",
    "    (animal,\n",
    "     date,\n",
    "     shank_pre,\n",
    "     cluster_pre,\n",
    "     shank_post,\n",
    "     cluster_post) = conn\n",
    "    df_conn_iv = df_btstrp_iv.loc[\n",
    "        (df_btstrp_iv['animal'] == animal) &\n",
    "        (df_btstrp_iv['date'] == date) &\n",
    "        (df_btstrp_iv['shank_pre'] == shank_pre) &\n",
    "        (df_btstrp_iv['cluster_pre'] == cluster_pre) &\n",
    "        (df_btstrp_iv['shank_post'] == shank_post) &\n",
    "        (df_btstrp_iv['cluster_post'] == cluster_post)]\n",
    "    med = np.nanmedian(df_conn_iv['ivwald'])\n",
    "    err_low = med-np.nanpercentile(df_conn_iv['ivwald'], perc_low)\n",
    "    err_high = np.nanpercentile(df_conn_iv['ivwald'], perc_high)-med\n",
    "    iv_med.append(med)\n",
    "    iv_err_low.append(err_low)\n",
    "    iv_err_high.append(err_high)\n",
    "\n",
    "\n",
    "    df_conn_cch = df_btstrp_cch.loc[\n",
    "        (df_btstrp_cch['animal'] == animal) &\n",
    "        (df_btstrp_cch['date'] == date) &\n",
    "        (df_btstrp_cch['shank_pre'] == shank_pre) &\n",
    "        (df_btstrp_cch['cluster_pre'] == cluster_pre) &\n",
    "        (df_btstrp_cch['shank_post'] == shank_post) &\n",
    "        (df_btstrp_cch['cluster_post'] == cluster_post)]\n",
    "    med = np.nanmedian(df_conn_cch['trans_prob'])\n",
    "    err_low = med-np.nanpercentile(df_conn_cch['trans_prob'], perc_low)\n",
    "    err_high = np.nanpercentile(df_conn_cch['trans_prob'], perc_high)-med\n",
    "    \n",
    "    cch_err_low.append(err_low)\n",
    "    cch_err_high.append(err_high)    \n",
    "    cch_med.append(med)\n",
    "    \n",
    "iv_err = np.vstack([iv_err_low, iv_err_high])\n",
    "iv_med = np.array(iv_med)\n",
    "cch_err = np.vstack([cch_err_low, cch_err_high])\n",
    "cch_mean = np.array(cch_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(4,4))\n",
    "\n",
    "#ax.set(xscale=\"log\", yscale=\"log\")\n",
    "x = cch_mean\n",
    "y = iv_mean\n",
    "ax.scatter(x,\n",
    "           y*0.1,\n",
    "           c='b',\n",
    "           alpha=0.6)\n",
    "ax.errorbar(x,\n",
    "            y*0.1,\n",
    "            xerr=cch_err,\n",
    "            yerr=iv_err*0.1,\n",
    "            fmt='None',\n",
    "            ecolor='b',\n",
    "            alpha=0.2)\n",
    "#ax.scatter(df_cch['transproball'],\n",
    "#           df_cch['ivwald'],\n",
    "#          c='r',\n",
    "#          alpha=0.1)\n",
    "ax.set_xlabel(r'CCH')\n",
    "ax.set_ylabel(r'IV $[10^{-1}]$')\n",
    "ax.set_xlim([-0.0001, 0.006])\n",
    "#ax.set_xlim([-0.0001, 0.01])\n",
    "ax.set_ylim([-0.0065, 0.025])\n",
    "ax.plot([0,1], [0,1], 'k--', alpha=0.3)\n",
    "despine(ax)\n",
    "plt.show()\n",
    "fig.tight_layout(rect=[0, 0.00, 1, 1])\n",
    "fig.savefig('manuscript/Optodata_comparisonIV_CCH.svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_max = np.argmax(x)\n",
    "print(x[id_max])\n",
    "print(y[id_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.vstack([iv_err_low, iv_err_high])*0.1\n",
    "a[0,0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_conn_cch['trans_prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_conn_iv['ivwald'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(df_conn_iv['ivwald'], 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(df_conn_iv['ivwald'], 97.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty and iv window length\n",
    "We observed that having a small window size, results in many units having very few spikes after stimulation.\n",
    "Therefore we expect that small window size results in higher errorbars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different iv windows for their error\n",
    "lst_iv_windows = np.array([1.5, 3., 4.5, 6., 7.5, 9.])*pq.s\n",
    "lst_df_diffwindw = []\n",
    "for windw in lst_iv_windows:\n",
    "    df_i = tls_exp.bootstrap_iv(\n",
    "            blks, btstrp_n,                                                                                                                                                                                                \n",
    "            df_sigstim,                                                                                                                                                                                                \n",
    "            iv_min_n_stim,                                                                                                                                                                                             \n",
    "            windw,                                                                                                                                                                                                 \n",
    "            iv_ltnc,\n",
    "            conns_sel=conns_sel)\n",
    "    lst_df_diffwindw.append(df_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_err = []\n",
    "for i in range(len(lst_iv_windows)):\n",
    "    df_i = lst_df_diffwindw[i]\n",
    "    iv_err = []\n",
    "    for conn in conns_sel:\n",
    "        (animal,\n",
    "         date,\n",
    "         shank_pre,\n",
    "         cluster_pre,\n",
    "         shank_post,\n",
    "         cluster_post) = conn\n",
    "        df_conn_iv = df_i.loc[\n",
    "            (df_i['animal'] == animal) &\n",
    "            (df_i['date'] == date) &\n",
    "            (df_i['shank_pre'] == shank_pre) &\n",
    "            (df_i['cluster_pre'] == cluster_pre) &\n",
    "            (df_i['shank_post'] == shank_post) &\n",
    "            (df_i['cluster_post'] == cluster_post)]\n",
    "        iv_err.append(np.nanstd(df_conn_iv['ivwald'])/np.sqrt(len(df_conn_iv['ivwald'])))\n",
    "    iv_err = np.array(iv_err)\n",
    "    mean_err.append(np.nanmean(iv_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(lst_iv_windows, mean_err)\n",
    "ax.set_title('Certainty of IV estimates against IV window length')\n",
    "ax.set_xlabel(r'IV time window [ms]')\n",
    "#ax.set_ylabel(r'Mean standard deviation of error')\n",
    "despine(ax)\n",
    "fig.savefig('manuscript/Optodata_error_by_IV_window.svg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
